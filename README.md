# Galaxea-Developer-Ecosystem
The OpenGalaxea developer ecosystem is continuously updated with new community projects and contributions.
Check back often to explore the latest tools, demos, and research powered by OpenGalaxea.
### ü§ñ Research & Papers
| Project Name                       | Tags     | Description                                                                          
|------------------------------------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [BEHAVIOR Robot Suite](https://behavior-robot-suite.github.io/) |#whole body control<br>#mobile manipulation | Developed by Stanford in collaboration with Galaxea, BRS leverages the **Galaxea R1 robot** to enable whole-body mobile manipulation for diverse household tasks. It has been accepted by **CoRL 2025**. |
| [DenseMatcher](https://tea-lab.github.io/DenseMatcher/) | #manipulation| Developed with the Galaxea **A1 robotic arm**, DenseMatcher enables cross-instance 3D semantic understanding, allowing robots to learn tasks like peeling a banana from a single human demonstration. This paper has been accepted by **ICLR 2025** as a Spotlight (Top 5%) paper. |
| [DemoGen](https://demo-generation.github.io/) | #manipulation                             | A demonstration generation system validated the Galaxea **R1 humanoid robot**. With only one real demonstration, it learns to place a banana into a basket and generalizes with high success across diverse positions and orientations. It was accepted by **RSS 2025**, and won the Best Long Paper Award at the SynData4CV Workshop, CVPR 2025. |
| [DemoSpeedup](https://demospeedup.github.io/)  | #mobile manipulation<br>#simulation       | Built on the Galaxea **R1 robot**, DemoSpeedup accelerates manipulation up to 3√ó faster. Using action entropy, it identifies "acceleration points" without manual labels, enabling efficient deployment across diverse scenarios. |
| [General Motion Retargeting](https://github.com/YanjieZe/GMR)| #simulation | Integrated with the Galaxea **R1 Pro robot**, GMR enables real-time high-quality whole-body retargeting for teleoperation (e.g., TWIST), supports multiple humanoid robots and human motion formats, and has been widely adopted in community research projects. |
| [VR-Robo](https://vr-robo.github.io/)| #Real-to-Sim-to-Real<br>#navigation<br>#simulation | Validated on the Galaxea **R1 robot**, VR-Robo introduces a "Real-to-Sim-to-Real" paradigm for autonomous navigation. Using only RGB input‚Äîwithout depth or LiDAR‚Äîit enables robust real-world navigation without requiring real-robot training. It has been accepted **at RA-L 2025**. |
| [Active Vision-Driven Robotic](https://avr-robot.github.io/) | #manipulation<br>#simulation| Built on the Galaxea **A1 robotic arm** and desktop teleoperation platform, AVR achieves stable tracking and precise control with repeatability under 1 cm accuracy.|
| [Pi0.5](https://www.physicalintelligence.company/blog/pi05) | #VLA | Validated on the Galaxea R1 Lite platform, œÄ-0.5 demonstrates robust embodied intelligence by leveraging its chassis and torso to handle complex household tasks with human-like flexibility. |
| [NavDP](https://wzcai99.github.io/navigation-diffusion-policy.github.io/)| #navigation<br>#Real-to-Sim-to-Real | Powered by the Galaxea **R1 robot**, NavDP enables autonomous navigation in unseen real-world environments. Trained entirely in simulation, it allows robots to avoid random obstacles and safely find their way. |
| [CogVLA](https://jiutian-vl.github.io/CogVLA-page/) | #VLA<br>#manipulation| A cognition-aligned Vision-Language-Action framework that uses instruction-driven routing and sparsification to improve efficiency and coherence. Demonstrated on the Galaxea R1 Lite for real-world tasks and accepted **at NeurIPS 2025**. |
| [Safebimanual](https://denghaoyuan123.github.io/SafeBimanip/) | #manipulation| A safety-aware framework for diffusion-based bimanual manipulation. By adding test-time trajectory optimization with dynamic constraints, it prevents unsafe behaviors and improves coordinated dual-arm performance on the Galaxea R1 robot and has been accepted **by CoRL 2025**. |
| [RoboChemist](https://zzongzheng0918.github.io/RoboChemist.github.io/)|  | #VLA<br>#manipulation| A dual-loop framework combining Vision-Language Models with Vision-Language-Action models for robotic chemistry. Using the Galaxea **R1 Robot**, it plans, guides, and monitors long-horizon lab tasks, enabling safe and precise execution of complex chemical protocols. It has been accepted **by CoRL 2025**. |


### üèÜ Competitions & Demos

| Competition/Demo Name       | Tags                             | Description                                                                                                                                                                                         
|----------------------------|-----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [WBCD2025-Champion](https://arxiv.org/abs/2506.06567)| #manipulation| The winning solution from Carnegie Mellon University, featured **at RSS 2025 workshop**, demonstrated the Galaxea **A1X robotic arm** to showcase state-of-the-art whole-body control.                                                                 |
| [BEHAVIOR Challenge](https://behavior.stanford.edu/)  | #dataset | Built on the Galaxea R1 Pro robot, the BEHAVIOR-1K dataset contains 50 full-length household tasks with 10,000 teleoperated demonstrations covering cooking, cleaning, rearrangement, and installation. The BEHAVIOR Challenge evaluates robots on reasoning, locomotion, and dexterous bimanual manipulation, co-hosted with **NeurIPS 2025**. |
