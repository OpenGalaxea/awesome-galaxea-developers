# Galaxea-Developer-Ecosystem
The OpenGalaxea developer ecosystem is continuously updated with new community projects and contributions. Check back often to explore the latest tools, demos, and research powered by OpenGalaxea.
**ü§ñ Research & Papers**

| **BEHAVIOR Robot Suite** | <https://behavior-robot-suite.github.io/> | #whole body control<br><br>#mobile manipulation | Developed by Stanford in collaboration with _Galaxea_, BRS leverages the _Galaxea_ **_R1 robot_** to enable whole-body mobile manipulation for diverse household tasks.It has been accepted by **CoRL 2025.** |
| --- | --- | --- | --- |
| **DenseMatcher** | <https://tea-lab.github.io/DenseMatcher/> | #manipulation | Developed with the _Galaxea_ **_A1 robotic arm_,** DenseMatcher enables cross-instance 3D semantic understanding, allowing robots to learn tasks like peeling a banana from a single human demonstration.This paper has been accepted by **ICLR 2025 as a Spotlight (Top 5%) paper.** |
| **DemoGen** | <https://demo-generation.github.io/> | #manipulation | A demonstration generation system validated the _Galaxea_ **_R1 humanoid robot_.** With only one real demonstration, it learns to place a banana into a basket and generalizes with high success across diverse positions and orientations.It was accepted by **RSS 2025**, and won **the Best Long Paper Award at the SynData4CV Workshop, CVPR 2025**. |
| **DemoSpeedup** | <https://demospeedup.github.io/> | #mobile manipulation<br><br>#simulation | Built on the _Galaxea_ **_R1_ robot,** DemoSpeedup accelerates manipulation up to 3√ó faster. Using action entropy, it identifies ‚Äúacceleration points‚Äù without manual labels, enabling efficient deployment across diverse scenarios. |
| **General Motion Retargeting** | <https://github.com/YanjieZe/GMR> | #simulation | Integrated with the _Galaxea_ **_R1 Pro_ robot**, GMR enables real-time high-quality whole-body retargeting for teleoperation (e.g., TWIST), supports multiple humanoid robots and human motion formats, and has been widely adopted in community research projects. |
| **VR-Robo** | <https://vr-robo.github.io/> | #Real-to-Sim-to-Real<br><br>#navigation<br><br>#simulation | Validated on the _Galaxea_ **_R1_ robot,** VR-Robo introduces a ‚ÄúReal-to-Sim-to-Real‚Äù paradigm for autonomous navigation. Using only RGB input‚Äîwithout depth or LiDAR‚Äîit enables robust real-world navigation without requiring real-robot training. It has been accepted at **RA-L 2025**. |
| **Active Vision-Driven Robotic** | <https://avr-robot.github.io/> | #manipulation<br><br>#simulation | Built on the _Galaxea_ **_A1 robotic arm_** and desktop teleoperation platform, AVR achieves stable tracking and precise control with repeatability under 1 cm accuracy. |
| **Pi0.5** | <https://www.physicalintelligence.company/blog/pi05> | #VLA | Validated on the **_Galaxea R1 Lite_** platform, œÄ-0.5 demonstrates robust embodied intelligence by leveraging its chassis and torso to handle complex household tasks with human-like flexibility. |
| **NavDP** | <https://wzcai99.github.io/navigation-diffusion-policy.github.io/> | #navigation<br><br>#Real-to-Sim-to-Real | Powered by the _Galaxea_ **_R1_ robot**, NavDP enables autonomous navigation in unseen real-world environments. Trained entirely in simulation, it allows robots to avoid random obstacles and safely find their way. |
| **CogVLA** | <https://jiutian-vl.github.io/CogVLA-page/> | #VLA<br><br>#manipulation | A cognition-aligned Vision-Language-Action framework that uses instruction-driven routing and sparsification to improve efficiency and coherence. Demonstrated on the **_Galaxea R1_ _Lite_** for real-world tasks and accepted **at NeurIPS 2025.** |
| **Safebimanual** | <https://denghaoyuan123.github.io/SafeBimanip/> | #manipulation | A safety-aware framework for diffusion-based bimanual manipulation. By adding test-time trajectory optimization with dynamic constraints, it prevents unsafe behaviors and improves coordinated dual-arm performance on the _Galaxea_ **_R1_ robot** and has been accepted by **CoRL 2025.** |
| **RoboChemist** | <https://zzongzheng0918.github.io/RoboChemist.github.io/> | #VLA<br><br>#manipulation | A dual-loop framework combining Vision-Language Models with Vision-Language-Action models for robotic chemistry. Using the _Galaxea_ **_R1 Robot_,** it plans, guides, and monitors long-horizon lab tasks, enabling safe and precise execution of complex chemical protocols.It has been accepted by **CoRL 2025.** |

**üèÜ Competitions & Demos**

| **WBCD2025-Champion** | <https://arxiv.org/abs/2506.06567> | #manipulation | The winning solution from Carnegie Mellon University, featured **at RSS 2025 workshop,** demonstrated the _Galaxea_ **_A1X robotic_** **_arm_** to showcase state-of-the-art whole-body control. |
| --- | --- | --- | --- |
| **BEHAVIOR Challenge** | <https://behavior.stanford.edu/> | #dataset | Built on the _Galaxea_ **_R1 Pro_ robot**, the BEHAVIOR-1K dataset contains 50 full-length household tasks with 10,000 teleoperated demonstrations covering cooking, cleaning, rearrangement, and installation. The BEHAVIOR Challenge evaluates robots on reasoning, locomotion, and dexterous bimanual manipulation, co-hosted with NeurIPS 2025. |
