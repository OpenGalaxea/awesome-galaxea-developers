# Awesome Galaxea Developers [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
The OpenGalaxea developer ecosystem is continuously updated with new community projects and contributions.
Check back often to explore the latest tools, demos, and research powered by OpenGalaxea.

### ü§ñ Research & Papers
| Project Name                       | Tags     | Description                                                                          
|------------------------------------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [BEHAVIOR Robot Suite](https://behavior-robot-suite.github.io/) |#whole body control<br>#mobile manipulation | Developed by Stanford in collaboration with Galaxea, BRS leverages the **Galaxea R1 robot** to enable whole-body mobile manipulation for diverse household tasks. It has been accepted by **CoRL 2025**. |
| [DenseMatcher](https://tea-lab.github.io/DenseMatcher/) | #manipulation| Developed with the Galaxea **A1 robotic arm**, DenseMatcher enables cross-instance 3D semantic understanding, allowing robots to learn tasks like peeling a banana from a single human demonstration. This paper has been accepted by **ICLR 2025** as a Spotlight (Top 5%) paper. |
| [DemoGen](https://demo-generation.github.io/) | #manipulation                             | A demonstration generation system validated the Galaxea **R1 humanoid robot**. With only one real demonstration, it learns to place a banana into a basket and generalizes with high success across diverse positions and orientations. It was accepted by **RSS 2025**, and won the Best Long Paper Award at the SynData4CV Workshop, CVPR 2025. |
| [DemoSpeedup](https://demospeedup.github.io/)  | #mobile manipulation<br>#simulation       | Built on the Galaxea **R1 robot**, DemoSpeedup accelerates manipulation up to 3√ó faster. Using action entropy, it identifies "acceleration points" without manual labels, enabling efficient deployment across diverse scenarios. |
| [General Motion Retargeting](https://github.com/YanjieZe/GMR)| #simulation #whole body control| Integrated with the Galaxea **R1 Pro robot**, GMR enables real-time high-quality whole-body retargeting for teleoperation (e.g., TWIST), supports multiple humanoid robots and human motion formats, and has been widely adopted in community research projects. |
| [VR-Robo](https://vr-robo.github.io/)| #Real-to-Sim-to-Real<br>#navigation<br>#simulation | Validated on the Galaxea **R1 robot**, VR-Robo introduces a "Real-to-Sim-to-Real" paradigm for autonomous navigation. Using only RGB input‚Äîwithout depth or LiDAR‚Äîit enables robust real-world navigation without requiring real-robot training. It has been accepted **at RA-L 2025**. |
| [Active Vision-Driven Robotic](https://avr-robot.github.io/) | #manipulation<br>#simulation| Built on the Galaxea **A1 robotic arm** and desktop teleoperation platform, AVR achieves stable tracking and precise control with repeatability under 1 cm accuracy.|
| [Pi0.5](https://www.physicalintelligence.company/blog/pi05) | #VLA | Validated on the Galaxea **R1 Lite** platform, œÄ-0.5 demonstrates robust embodied intelligence by leveraging its chassis and torso to handle complex household tasks with human-like flexibility. |
| [NavDP](https://wzcai99.github.io/navigation-diffusion-policy.github.io/)| #navigation<br>#Real-to-Sim-to-Real | Powered by the Galaxea **R1 robot**, NavDP enables autonomous navigation in unseen real-world environments. Trained entirely in simulation, it allows robots to avoid random obstacles and safely find their way. |
| [CogVLA](https://jiutian-vl.github.io/CogVLA-page/) | #VLA<br>#manipulation| A cognition-aligned Vision-Language-Action framework that uses instruction-driven routing and sparsification to improve efficiency and coherence. Demonstrated on the Galaxea **R1 Lite** for real-world tasks and accepted **at NeurIPS 2025**. |
| [Safebimanual](https://denghaoyuan123.github.io/SafeBimanip/) | #manipulation| A safety-aware framework for diffusion-based bimanual manipulation. By adding test-time trajectory optimization with dynamic constraints, it prevents unsafe behaviors and improves coordinated dual-arm performance on the Galaxea **R1 robot** and has been accepted **by CoRL 2025**. |
| [XRoboToolkit](https://xr-robotics.github.io/)|#robot teleoperation<br>#manipulation| XRoboToolkit is an OpenXR-based framework for robot teleoperation in extended reality. It provides low-latency feedback, precise motion control, and flexible tracking across devices. The system is fully compatible with the Galaxea **R1 Lite** and other platforms, enabling scalable and high-quality demonstration data collection for Vision-Language-Action models. |
| [HERMES](https://gemcollector.github.io/HERMES/)|#mobile manipulation|HERMES is a human-to-robot learning framework for mobile bimanual dexterous manipulation. Built on the **Galaxea A1** arms and **X1** mobile base, it translates human motion into physically plausible robot behaviors via reinforcement learning. With sim2real transfer and PnP-based localization, HERMES achieves robust, generalizable performance in real-world environments. |
| [RoboBrain-X0](https://superrobobrain.github.io/)|#mobile manipulation|A cross-embodiment foundation model from BAAI that unifies vision, language, and action for zero-shot generalization and few-shot adaptation. Deployed on the Galaxea **R1 Lite** for real-world task execution.|
| [MoMaGen](https://momagen-rss.github.io/)|#simulation #mobile manipulation|A large-scale data generation framework for bimanual mobile manipulation, addressing challenges in base placement and camera positioning for visuomotor learning. Using the Galaxea **R1 robot**, MoMaGen formulates data generation as a constrained optimization problem that ensures both reachability and visibility, enabling diverse datasets and robust imitation learning from a single demonstration.|
| [DAWN](https://arxiv.org/abs/2509.22652)| #mobile manipulation| DAWN is a novel framework for translating language instructions into robotic actions. It integrates high-level planning and low-level control into a unified diffusion process, operating within the pixel-motion space. The framework accepts language instructions (with optional visual context), generates a "pixel motion map," and converts it into robotic actions. This system has been deployed on the Galaxea **R1 Lite** platform. The pixel-based approach not only endows motion flows with interpretability but also enhances generalization across tasks.|
| [iFlyBot-VLA](https://arxiv.org/abs/2511.01914)|#VLA #mobile manipulation |iFlyBot-VLA, with **Galaxea Open-World Dataset** included in pre-training, adopts dual-level action representation and mixed training, achieving excellent performance in simulation and real-world manipulation.|
| [UNICOD](https://arxiv.org/abs/2510.10642)| #simulation #mobile manipulation |Trained on diverse datasets including **Galaxea Open-World** and its VQA data, integrating discrete and continuous representations via two-stage learning to achieve SOTA in simulation and real-world scenarios.|
| [FASTER](https://arxiv.org/abs/2510.13375)| #VLA  #mobile manipulation |FASTer VLA model to optimize autoregressive VLA performance and inference efficiency; Galaxea **R1 Lite** hardware is used for real-world task testing to verify the model's actual operation performance.|
| [Depth VLA](https://arxiv.org/abs/2510.13375)| #VLA #mobile manipulation |DepthVLA integrates a depth expert, implemented on the Galaxea **R1 Lite** , significantly enhancing the robot's spatial reasoning and operational accuracy.|
| [Robocoin](https://flagopen.github.io/RoboCOIN/)| #dataset|RoboCOIN is currently the world‚Äôs largest and most comprehensive real-world bimanual robot manipulation dataset. It is collected across diverse real-world environments using 15 heterogeneous robot platforms, and contains 180,000+ real trajectories and 421 tasks. The dataset integrates commonly used platforms from research, industry, and education ‚Äî including mobile manipulation systems such as the Galaxea **R1 Lite**, which were used to collect a wide range of everyday interaction behaviors.|

### üèÜ Competitions & Demos

| Competition/Demo Name       | Tags                             | Description                                                                                                                                                                                         
|----------------------------|-----------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [WBCD2025-Champion](https://arxiv.org/abs/2506.06567)| #manipulation| The winning solution from Carnegie Mellon University, featured **at RSS 2025 workshop**, demonstrated the Galaxea **A1X robotic arm** to showcase state-of-the-art whole-body control.                                                                 |
| [BEHAVIOR Challenge](https://behavior.stanford.edu/)  | #dataset | Built on the Galaxea **R1 Pro robot**, the BEHAVIOR-1K dataset contains 50 full-length household tasks with 10,000 teleoperated demonstrations covering cooking, cleaning, rearrangement, and installation. The BEHAVIOR Challenge evaluates robots on reasoning, locomotion, and dexterous bimanual manipulation, co-hosted with **NeurIPS 2025**. |
| [RoCo Challenge](https://rocochallenge.github.io/RoCo2026/)| #simulation #mobile manipulation |RoCo (Robotic Collaboration) is an industry-oriented benchmark initiated by NTU and presented at **AAAI 2026**, focusing on human-in-the-loop gearbox assembly across three core scenarios. It supports multiple robotic platforms‚Äîincluding the Galaxea **R1 Pro**‚Äîto advance adaptive collaboration and error-aware autonomy in realistic manufacturing settings.|
